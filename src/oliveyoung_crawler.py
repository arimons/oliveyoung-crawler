"""
ì˜¬ë¦¬ë¸Œì˜ í†µí•© í¬ë¡¤ëŸ¬
ê²€ìƒ‰, ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘, ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ í†µí•©
"""
import os
import sys

# src í´ë”ë¥¼ Python pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from crawler_selenium import OliveyoungCrawler
from product_detail_crawler import ProductDetailCrawler
from review_crawler import ReviewCrawler
import json
from datetime import datetime
from typing import Dict, List
import time


class OliveyoungIntegratedCrawler:
    """ì˜¬ë¦¬ë¸Œì˜ í†µí•© í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = True):
        """
        Args:
            headless: ë¸Œë¼ìš°ì € ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì—¬ë¶€
        """
        self.base_crawler = OliveyoungCrawler(headless=headless)
        self.detail_crawler = None
        self.review_crawler = None

    def start(self):
        """í¬ë¡¤ëŸ¬ ì‹œì‘"""
        self.base_crawler.start()
        self.detail_crawler = ProductDetailCrawler(self.base_crawler.driver)
        self.review_crawler = ReviewCrawler(self.base_crawler.driver)

    def stop(self):
        """í¬ë¡¤ëŸ¬ ì¢…ë£Œ"""
        self.base_crawler.stop()

    def create_product_folder(self, product_name: str) -> str:
        """
        ìƒí’ˆë³„ í´ë” ìƒì„±

        Args:
            product_name: ìƒí’ˆëª…

        Returns:
            ìƒì„±ëœ í´ë” ê²½ë¡œ
        """
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_name = "".join(c for c in product_name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', ' ')  # ê³µë°± ìœ ì§€

        # ë‚ ì§œë§Œ ì¶”ê°€ (YYMMDD í˜•ì‹)
        date_str = datetime.now().strftime("%y%m%d")
        folder_name = f"{date_str}_{safe_name}"

        folder_path = os.path.join("data", folder_name)
        os.makedirs(folder_path, exist_ok=True)

        print(f"ğŸ“ í´ë” ìƒì„±: {folder_path}")
        return folder_path

    def search_and_get_first_product(self, keyword: str) -> Dict:
        """
        ê²€ìƒ‰í•˜ê³  ì²« ë²ˆì§¸ ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            ìƒí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        # í™ˆí˜ì´ì§€ ì ‘ì†
        self.base_crawler.navigate_to_home()

        # ê²€ìƒ‰
        self.base_crawler.search_product(keyword)

        # ì²« ë²ˆì§¸ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        products = self.base_crawler.extract_product_info(max_products=1)

        if not products:
            raise Exception("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

        return products[0]

    def crawl_product_detail_by_url(self, product_url: str, save_folder: str) -> Dict:
        """
        URLë¡œ ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§

        Args:
            product_url: ìƒí’ˆ URL
            save_folder: ì €ì¥ í´ë” ê²½ë¡œ

        Returns:
            ìƒí’ˆ ì •ë³´ ë° ì´ë¯¸ì§€ ê²½ë¡œ
        """
        print(f"\n{'='*60}")
        print(f"ìƒí’ˆ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*60}")

        # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
        self.detail_crawler.go_to_product_detail(product_url)

        # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        product_info = self.detail_crawler.extract_product_info_from_detail()

        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
        self.detail_crawler.click_more_button()

        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        image_urls = self.detail_crawler.extract_product_images()

        if not image_urls:
            print("âš ï¸  ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            product_info["ì´ë¯¸ì§€_ê²½ë¡œ"] = ""
            product_info["ì´ë¯¸ì§€_ê°œìˆ˜"] = 0
            return product_info

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë³‘í•©
        output_image_path = os.path.join(save_folder, "product_detail_merged.jpg")
        saved_path = self.detail_crawler.download_and_merge_images(image_urls, output_image_path)

        product_info["ì´ë¯¸ì§€_ê²½ë¡œ"] = saved_path
        product_info["ì´ë¯¸ì§€_ê°œìˆ˜"] = len(image_urls)
        product_info["ìˆ˜ì§‘ì‹œê°"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        return product_info

    def crawl_product_by_keyword(self, keyword: str, save_format: str = "json") -> Dict:
        """
        í‚¤ì›Œë“œë¡œ ìƒí’ˆ ê²€ìƒ‰ ë° í¬ë¡¤ë§

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"í‚¤ì›Œë“œ í¬ë¡¤ë§: {keyword}")
        print(f"{'='*60}\n")

        # ê²€ìƒ‰ ë° ì²« ë²ˆì§¸ ìƒí’ˆ ê°€ì ¸ì˜¤ê¸°
        first_product = self.search_and_get_first_product(keyword)
        product_url = first_product["URL"]

        # í´ë” ìƒì„±
        product_name = first_product["ìƒí’ˆëª…"].split('\n')[0][:50]  # ìƒí’ˆëª… ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        save_folder = self.create_product_folder(product_name)

        # ìƒì„¸ í¬ë¡¤ë§
        product_info = self.crawl_product_detail_by_url(product_url, save_folder)

        # ë°ì´í„° ì €ì¥
        self.save_product_info(product_info, save_folder, save_format)

        result = {
            "ìƒí’ˆëª…": product_info.get("ìƒí’ˆëª…", ""),
            "í´ë”": save_folder,
            "ì´ë¯¸ì§€": product_info.get("ì´ë¯¸ì§€_ê²½ë¡œ", ""),
            "ì´ë¯¸ì§€_ê°œìˆ˜": product_info.get("ì´ë¯¸ì§€_ê°œìˆ˜", 0)
        }

        return result

    def crawl_product_by_url(self, product_url: str, product_name: str = None, save_format: str = "json") -> Dict:
        """
        URLë¡œ ìƒí’ˆ í¬ë¡¤ë§

        Args:
            product_url: ìƒí’ˆ URL
            product_name: ìƒí’ˆëª… (í´ë”ëª… ìƒì„±ìš©, Noneì´ë©´ ìë™ ì¶”ì¶œ)
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"URL í¬ë¡¤ë§")
        print(f"{'='*60}\n")

        # í´ë” ìƒì„± (ì„ì‹œ ì´ë¦„)
        if not product_name:
            product_name = f"product_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        save_folder = self.create_product_folder(product_name)

        # ìƒì„¸ í¬ë¡¤ë§
        product_info = self.crawl_product_detail_by_url(product_url, save_folder)

        # ì‹¤ì œ ìƒí’ˆëª…ìœ¼ë¡œ í´ë” ì´ë¦„ ë³€ê²½
        if product_info.get("ìƒí’ˆëª…") and product_info["ìƒí’ˆëª…"] != "ì •ë³´ ì—†ìŒ":
            actual_name = product_info["ìƒí’ˆëª…"].split('\n')[0][:50]
            new_folder = self.create_product_folder(actual_name)

            # íŒŒì¼ ì´ë™
            import shutil
            if os.path.exists(save_folder):
                for file in os.listdir(save_folder):
                    shutil.move(
                        os.path.join(save_folder, file),
                        os.path.join(new_folder, file)
                    )
                os.rmdir(save_folder)
                save_folder = new_folder
                product_info["ì´ë¯¸ì§€_ê²½ë¡œ"] = os.path.join(new_folder, "product_detail_merged.jpg")

        # ë°ì´í„° ì €ì¥
        self.save_product_info(product_info, save_folder, save_format)

        result = {
            "ìƒí’ˆëª…": product_info.get("ìƒí’ˆëª…", ""),
            "í´ë”": save_folder,
            "ì´ë¯¸ì§€": product_info.get("ì´ë¯¸ì§€_ê²½ë¡œ", ""),
            "ì´ë¯¸ì§€_ê°œìˆ˜": product_info.get("ì´ë¯¸ì§€_ê°œìˆ˜", 0)
        }

        return result

    def save_product_info(self, product_info: Dict, save_folder: str, save_format: str):
        """
        ìƒí’ˆ ì •ë³´ ì €ì¥

        Args:
            product_info: ìƒí’ˆ ì •ë³´
            save_folder: ì €ì¥ í´ë”
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)
        """
        if save_format in ["json", "both"]:
            json_path = os.path.join(save_folder, "product_info.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(product_info, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSON ì €ì¥: {json_path}")

        if save_format in ["csv", "both"]:
            import pandas as pd
            csv_path = os.path.join(save_folder, "product_info.csv")
            df = pd.DataFrame([product_info])
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")
