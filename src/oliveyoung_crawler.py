"""
ì˜¬ë¦¬ë¸Œì˜ í†µí•© í¬ë¡¤ëŸ¬
ê²€ìƒ‰, ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘, ìƒì„¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ í†µí•©
"""
import os
import sys

# src í´ë”ë¥¼ Python pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from crawler_selenium import OliveyoungCrawler
from product_detail_crawler import ProductDetailCrawler
from review_crawler import ReviewCrawler
import json
from datetime import datetime
from typing import Dict, List
import time


class OliveyoungIntegratedCrawler:
    """ì˜¬ë¦¬ë¸Œì˜ í†µí•© í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = True, log_callback=None):
        """
        Args:
            headless: ë¸Œë¼ìš°ì € ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì—¬ë¶€
            log_callback: ë¡œê·¸ ì¶œë ¥ ì½œë°± í•¨ìˆ˜ (optional)
        """
        self.base_crawler = OliveyoungCrawler(headless=headless)
        self.detail_crawler = None
        self.review_crawler = None
        self.log_callback = log_callback

    def log(self, message: str):
        """ë¡œê·¸ ì¶œë ¥"""
        print(message)
        if self.log_callback:
            self.log_callback(message)

    def start(self):
        """í¬ë¡¤ëŸ¬ ì‹œì‘"""
        self.base_crawler.start()
        self.detail_crawler = ProductDetailCrawler(self.base_crawler.driver, log_callback=self.log_callback)
        self.review_crawler = ReviewCrawler(self.base_crawler.driver, log_callback=self.log_callback)

    def stop(self):
        """í¬ë¡¤ëŸ¬ ì¢…ë£Œ"""
        self.base_crawler.stop()
        self.detail_crawler = None
        self.review_crawler = None

    def create_product_folder(self, product_name: str) -> str:
        """
        ìƒí’ˆë³„ í´ë” ìƒì„±

        Args:
            product_name: ìƒí’ˆëª…

        Returns:
            ìƒì„±ëœ í´ë” ê²½ë¡œ
        """
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        safe_name = "".join(c for c in product_name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', ' ')  # ê³µë°± ìœ ì§€

        # ë‚ ì§œë§Œ ì¶”ê°€ (YYMMDD í˜•ì‹)
        date_str = datetime.now().strftime("%y%m%d")
        folder_name = f"{date_str}_{safe_name}"

        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ì„¤ì •
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(project_root, "data")
        
        folder_path = os.path.join(data_dir, folder_name)
        os.makedirs(folder_path, exist_ok=True)

        print(f"ğŸ“ í´ë” ìƒì„±: {folder_path}")
        return folder_path

    def search_and_get_first_product(self, keyword: str) -> Dict:
        """
        ê²€ìƒ‰í•˜ê³  ì²« ë²ˆì§¸ ìƒí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            ìƒí’ˆ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        # í™ˆí˜ì´ì§€ ì ‘ì†
        self.base_crawler.navigate_to_home()

        # ê²€ìƒ‰
        self.base_crawler.search_product(keyword)

        # ì²« ë²ˆì§¸ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        products = self.base_crawler.extract_product_info(max_products=1)

        if not products:
            raise Exception("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

        return products[0]

    def crawl_product_detail_by_url(self, product_url: str, save_folder: str, split_mode: str = "aggressive", collect_reviews: bool = False, review_end_date: str = None, reviews_only: bool = False) -> Dict:
        """
        URLë¡œ ìƒí’ˆ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§

        Args:
            product_url: ìƒí’ˆ URL
            save_folder: ì €ì¥ í´ë” ê²½ë¡œ
            split_mode: ì´ë¯¸ì§€ ë¶„í•  ëª¨ë“œ (conservative, aggressive, tile)
            collect_reviews: ë¦¬ë·° ìˆ˜ì§‘ ì—¬ë¶€
            review_end_date: ë¦¬ë·° ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ (YYYY.MM.DD)
            reviews_only: ë¦¬ë·°ë§Œ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ê±´ë„ˆë›°ê¸°)

        Returns:
            ìƒí’ˆ ì •ë³´ ë° ì´ë¯¸ì§€ ê²½ë¡œ
        """
        print(f"\n{'='*60}")
        print(f"ìƒí’ˆ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*60}")

        # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
        self.detail_crawler.go_to_product_detail(product_url)

        # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        product_info = self.detail_crawler.extract_product_info_from_detail()

        # ì´ë¯¸ì§€ ìˆ˜ì§‘ (reviews_onlyê°€ Falseì¼ ë•Œë§Œ)
        if not reviews_only:
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
            self.detail_crawler.click_more_button()

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_urls = self.detail_crawler.extract_product_images()

            if not image_urls:
                print("âš ï¸  ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
                product_info["ì´ë¯¸ì§€_ê²½ë¡œ"] = ""
                product_info["ì´ë¯¸ì§€_ê°œìˆ˜"] = 0
            else:
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë³‘í•©
                output_image_path = os.path.join(save_folder, "product_detail_merged.jpg")
                self.detail_crawler.download_and_merge_images(image_urls, output_image_path, split_mode=split_mode)

                # ì¸ë„¤ì¼ ë‹¤ìš´ë¡œë“œ
                if product_info.get("ì¸ë„¤ì¼_URL"):
                    try:
                        import requests
                        thumb_url = product_info["ì¸ë„¤ì¼_URL"]
                        thumb_path = os.path.join(save_folder, "thumbnail.jpg")
                        
                        response = requests.get(thumb_url, stream=True)
                        if response.status_code == 200:
                            with open(thumb_path, 'wb') as f:
                                for chunk in response.iter_content(1024):
                                    f.write(chunk)
                            print(f"  ğŸ–¼ï¸ ì¸ë„¤ì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {thumb_path}")
                            product_info["ì¸ë„¤ì¼_ê²½ë¡œ"] = thumb_path
                        else:
                            print(f"  âš ï¸ ì¸ë„¤ì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (Status: {response.status_code})")
                    except Exception as e:
                        print(f"  âš ï¸ ì¸ë„¤ì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            print("ğŸ“ ë¦¬ë·°ë§Œ ìˆ˜ì§‘ ëª¨ë“œ: ì´ë¯¸ì§€ ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸°")
            product_info["ì´ë¯¸ì§€_ê²½ë¡œ"] = ""
            product_info["ì´ë¯¸ì§€_ê°œìˆ˜"] = 0

        product_info["ìˆ˜ì§‘ì‹œê°"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # ë¦¬ë·° ë©”íƒ€ë°ì´í„° (ë³„ì , ë¦¬ë·°ìˆ˜) ì¶”ì¶œ
        review_meta = self.detail_crawler.extract_review_metadata()
        if review_meta:
            product_info.update(review_meta)

        # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ì‚¬ìš©ì ìš”ì²­: ìƒí’ˆì •ë³´ íƒ­ì˜ íŠ¹ì • í–‰)
        specific_info = self.detail_crawler.extract_specific_info()
        if specific_info:
            product_info.update(specific_info)

        # ë¦¬ë·° í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì˜µì…˜)
        if collect_reviews:
            print(f"ğŸ“ ë¦¬ë·° í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘... (ì¢…ë£Œì¼: {review_end_date})")
            review_file = os.path.join(save_folder, "reviews.txt")
            
            # New Layout (Infinite Scroll) ì‹œë„
            try:
                review_count = self.review_crawler.crawl_reviews_infinite_scroll(
                    output_path=review_file,
                    end_date=review_end_date
                )
            except Exception as e:
                print(f"âš ï¸ ë¬´í•œ ìŠ¤í¬ë¡¤ ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‹œë„: {e}")
                review_count = self.review_crawler.crawl_all_reviews(
                    output_path=review_file,
                    end_date=review_end_date
                )
                
            product_info["ìˆ˜ì§‘ëœ_ë¦¬ë·°_ê°œìˆ˜"] = review_count


        return product_info

    def crawl_product_by_keyword(self, keyword: str, save_format: str = "json", split_mode: str = "aggressive", collect_reviews: bool = False, review_end_date: str = None, reviews_only: bool = False) -> Dict:
        """
        í‚¤ì›Œë“œë¡œ ìƒí’ˆ ê²€ìƒ‰ ë° í¬ë¡¤ë§

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)
            split_mode: ì´ë¯¸ì§€ ë¶„í•  ëª¨ë“œ
            collect_reviews: ë¦¬ë·° ìˆ˜ì§‘ ì—¬ë¶€
            review_end_date: ë¦¬ë·° ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ
            reviews_only: ë¦¬ë·°ë§Œ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ê±´ë„ˆë›°ê¸°)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"í‚¤ì›Œë“œ í¬ë¡¤ë§: {keyword}")
        print(f"{'='*60}\n")

        # ê²€ìƒ‰ ë° ì²« ë²ˆì§¸ ìƒí’ˆ ê°€ì ¸ì˜¤ê¸°
        first_product = self.search_and_get_first_product(keyword)
        product_url = first_product["URL"]

        # í´ë” ìƒì„±
        product_name = first_product.get("ìƒí’ˆëª…", "Unknown")
        if product_name:
            product_name = product_name.split('\n')[0][:50]  # ìƒí’ˆëª… ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        else:
            product_name = "Unknown"
        save_folder = self.create_product_folder(product_name)

        # ìƒì„¸ í¬ë¡¤ë§
        product_info = self.crawl_product_detail_by_url(product_url, save_folder, split_mode=split_mode, collect_reviews=collect_reviews, review_end_date=review_end_date, reviews_only=reviews_only)

        # ë°ì´í„° ì €ì¥
        self.save_product_info(product_info, save_folder, save_format)

        result = {
            "ìƒí’ˆëª…": product_info.get("ìƒí’ˆëª…", ""),
            "í´ë”": save_folder,
            "ì´ë¯¸ì§€": product_info.get("ì´ë¯¸ì§€_ê²½ë¡œ", ""),
            "ì´ë¯¸ì§€_ê°œìˆ˜": product_info.get("ì´ë¯¸ì§€_ê°œìˆ˜", 0)
        }

        return result

    def crawl_product_by_url(self, product_url: str, product_name: str = None, save_format: str = "json", split_mode: str = "aggressive", collect_reviews: bool = False, review_end_date: str = None, reviews_only: bool = False) -> Dict:
        """
        URLë¡œ ìƒí’ˆ í¬ë¡¤ë§

        Args:
            product_url: ìƒí’ˆ URL
            product_name: ìƒí’ˆëª… (í´ë”ëª… ìƒì„±ìš©, Noneì´ë©´ ìë™ ì¶”ì¶œ)
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)
            split_mode: ì´ë¯¸ì§€ ë¶„í•  ëª¨ë“œ
            collect_reviews: ë¦¬ë·° ìˆ˜ì§‘ ì—¬ë¶€
            review_end_date: ë¦¬ë·° ìˆ˜ì§‘ ì¢…ë£Œ ë‚ ì§œ
            reviews_only: ë¦¬ë·°ë§Œ ìˆ˜ì§‘ (ì´ë¯¸ì§€ ê±´ë„ˆë›°ê¸°)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"URL í¬ë¡¤ë§")
        print(f"{'='*60}\n")

        # ìƒí’ˆëª…ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°, í˜ì´ì§€ì—ì„œ ë¨¼ì € ì¶”ì¶œ
        if not product_name:
            print("â„¹ï¸  ìƒí’ˆëª…ì„ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤...")
            # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
            self.detail_crawler.go_to_product_detail(product_url)
            # ìƒí’ˆ ì •ë³´ì—ì„œ ìƒí’ˆëª…ë§Œ ë¨¼ì € ì¶”ì¶œ
            temp_info = self.detail_crawler.extract_product_info_from_detail()
            product_name = temp_info.get("ìƒí’ˆëª…", "Unknown")
            
            if product_name and product_name != "Unknown":
                # ìƒí’ˆëª… ì •ë¦¬ (ì²« ì¤„, ìµœëŒ€ 50ì)
                product_name = product_name.split('\n')[0][:50]
                print(f"âœ… ìƒí’ˆëª… ì¶”ì¶œ ì™„ë£Œ: {product_name}")
            else:
                product_name = f"product_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                print(f"âš ï¸  ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì„ì‹œ ì´ë¦„ ì‚¬ìš©: {product_name}")

        # í´ë” ìƒì„± (ì´ì œ ì‹¤ì œ ìƒí’ˆëª…ìœ¼ë¡œ)
        save_folder = self.create_product_folder(product_name)

        # ìƒì„¸ í¬ë¡¤ë§
        product_info = self.crawl_product_detail_by_url(product_url, save_folder, split_mode=split_mode, collect_reviews=collect_reviews, review_end_date=review_end_date, reviews_only=reviews_only)

        # ë°ì´í„° ì €ì¥
        self.save_product_info(product_info, save_folder, save_format)

        result = {
            "ìƒí’ˆëª…": product_info.get("ìƒí’ˆëª…", ""),
            "í´ë”": save_folder,
            "ì´ë¯¸ì§€": product_info.get("ì´ë¯¸ì§€_ê²½ë¡œ", ""),
            "ì´ë¯¸ì§€_ê°œìˆ˜": product_info.get("ì´ë¯¸ì§€_ê°œìˆ˜", 0)
        }

        return result

    def save_product_info(self, product_info: Dict, save_folder: str, save_format: str):
        """
        ìƒí’ˆ ì •ë³´ ì €ì¥

        Args:
            product_info: ìƒí’ˆ ì •ë³´
            save_folder: ì €ì¥ í´ë”
            save_format: ì €ì¥ í˜•ì‹ (json/csv/both)
        """
        if save_format in ["json", "both"]:
            json_path = os.path.join(save_folder, "product_info.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(product_info, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSON ì €ì¥: {json_path}")

        if save_format in ["csv", "both"]:
            import pandas as pd
            csv_path = os.path.join(save_folder, "product_info.csv")
            df = pd.DataFrame([product_info])
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")
