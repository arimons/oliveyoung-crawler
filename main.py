"""
ì˜¬ë¦¬ë¸Œì˜ í¬ë¡¤ëŸ¬ - ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python main.py

ì‹¤í–‰í•˜ë©´ ëŒ€í™”í˜•ìœ¼ë¡œ ê²€ìƒ‰ì–´ì™€ ì˜µì…˜ì„ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
import sys
sys.path.append('src')

from crawler_selenium import OliveyoungCrawler


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ›’ ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ëŸ¬")
    print("=" * 60)

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    search_keyword = input("\nğŸ” ê²€ìƒ‰í•  ìƒí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not search_keyword:
        print("âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    try:
        max_products = int(input(f"ğŸ“Š ì¶”ì¶œí•  ìƒí’ˆ ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): ").strip() or "10")
    except ValueError:
        max_products = 10
        print(f"âš ï¸  ì˜¬ë°”ë¥¸ ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤. ê¸°ë³¸ê°’ {max_products}ê°œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")

    # headless ëª¨ë“œ ì„ íƒ
    show_browser = input("\nğŸ‘€ ë¸Œë¼ìš°ì €ë¥¼ í™”ë©´ì— í‘œì‹œí• ê¹Œìš”? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower()
    headless = show_browser != 'y'

    # ì €ì¥ í˜•ì‹ ì„ íƒ
    save_format = input("ğŸ’¾ ì €ì¥ í˜•ì‹ì„ ì„ íƒí•˜ì„¸ìš” (json/csv/both, ê¸°ë³¸ê°’: both): ").strip().lower()
    if save_format not in ['json', 'csv', 'both']:
        save_format = 'both'

    print("\n" + "=" * 60)
    print("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)

    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = OliveyoungCrawler(headless=headless)

    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        crawler.start()

        # í™ˆí˜ì´ì§€ ì ‘ì†
        crawler.navigate_to_home()

        # ì œí’ˆ ê²€ìƒ‰
        crawler.search_product(search_keyword)

        # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        products = crawler.extract_product_info(max_products=max_products)

        if products:
            print(f"\nâœ… ì¶”ì¶œ ì„±ê³µ! ì´ {len(products)}ê°œ ìƒí’ˆ")

            # ë°ì´í„° ì €ì¥
            saved_files = []

            if save_format in ['json', 'both']:
                json_file = crawler.save_to_json(products, f"oliveyoung_{search_keyword}")
                saved_files.append(json_file)

            if save_format in ['csv', 'both']:
                csv_file = crawler.save_to_csv(products, f"oliveyoung_{search_keyword}")
                saved_files.append(csv_file)

            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "=" * 60)
            print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print("=" * 60)
            print(f"ê²€ìƒ‰ì–´: {search_keyword}")
            print(f"ì¶”ì¶œ ìƒí’ˆ ìˆ˜: {len(products)}ê°œ")
            print(f"ì €ì¥ëœ íŒŒì¼:")
            for file in saved_files:
                print(f"  - {file}")
            print("=" * 60)

        else:
            print("\nâš ï¸  ì¶”ì¶œëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\në¬¸ì œê°€ ê³„ì†ë˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
        print("  1. ì¸í„°ë„· ì—°ê²° ìƒíƒœ")
        print("  2. ì˜¬ë¦¬ë¸Œì˜ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€")
        print("  3. Chrome ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì—¬ë¶€")

        import traceback
        traceback.print_exc()

    finally:
        crawler.stop()


if __name__ == "__main__":
    main()
